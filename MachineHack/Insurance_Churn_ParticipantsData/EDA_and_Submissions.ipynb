{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This notebook is Final submission for the hackathon \n",
    "\n",
    "https://www.machinehack.com/hackathons/insurance_churn_prediction_weekend_hackathon_2/overview\n",
    "\n",
    "Alert: This notebook is an old notebook so I couldn't structure the preprocessing and model code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.set_option(\"display.max_columns\", 200)\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import SGD\n",
    "from keras import optimizers\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers.convolutional import Conv1D, MaxPooling1D\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import keras\n",
    "from keras.layers import LeakyReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('Train.csv')\n",
    "test = pd.read_csv('Test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    29941\n",
       "1     3967\n",
       "Name: labels, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['labels'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   labels\n",
       "0       1\n",
       "1       1\n",
       "2       1\n",
       "3       1\n",
       "4       1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_excel('sample_submission.xlsx').head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 33908 entries, 0 to 33907\n",
      "Data columns (total 17 columns):\n",
      "feature_0     33908 non-null float64\n",
      "feature_1     33908 non-null float64\n",
      "feature_2     33908 non-null float64\n",
      "feature_3     33908 non-null float64\n",
      "feature_4     33908 non-null float64\n",
      "feature_5     33908 non-null float64\n",
      "feature_6     33908 non-null float64\n",
      "feature_7     33908 non-null int64\n",
      "feature_8     33908 non-null int64\n",
      "feature_9     33908 non-null int64\n",
      "feature_10    33908 non-null int64\n",
      "feature_11    33908 non-null int64\n",
      "feature_12    33908 non-null int64\n",
      "feature_13    33908 non-null int64\n",
      "feature_14    33908 non-null int64\n",
      "feature_15    33908 non-null int64\n",
      "labels        33908 non-null int64\n",
      "dtypes: float64(7), int64(10)\n",
      "memory usage: 4.4 MB\n"
     ]
    }
   ],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 11303 entries, 0 to 11302\n",
      "Data columns (total 16 columns):\n",
      "feature_0     11303 non-null float64\n",
      "feature_1     11303 non-null float64\n",
      "feature_2     11303 non-null float64\n",
      "feature_3     11303 non-null float64\n",
      "feature_4     11303 non-null float64\n",
      "feature_5     11303 non-null float64\n",
      "feature_6     11303 non-null float64\n",
      "feature_7     11303 non-null int64\n",
      "feature_8     11303 non-null int64\n",
      "feature_9     11303 non-null int64\n",
      "feature_10    11303 non-null int64\n",
      "feature_11    11303 non-null int64\n",
      "feature_12    11303 non-null int64\n",
      "feature_13    11303 non-null int64\n",
      "feature_14    11303 non-null int64\n",
      "feature_15    11303 non-null int64\n",
      "dtypes: float64(7), int64(9)\n",
      "memory usage: 1.4 MB\n"
     ]
    }
   ],
   "source": [
    "test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(train.drop('labels', axis=1), train['labels'],\n",
    "                                                    test_size=0.30, stratify=train['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.losses import binary_crossentropy, categorical_crossentropy\n",
    "import keras.backend as K\n",
    "\n",
    "def f1(y_true, y_pred):\n",
    "    y_pred = K.round(y_pred)\n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return K.mean(f1)\n",
    "\n",
    "def f1_loss(y_true, y_pred):\n",
    "    \n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return 1 - K.mean(f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0425 14:09:45.023117 4501493184 deprecation.py:323] From <ipython-input-8-32aab2f104de>:16: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 60)                1020      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 35)                2135      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 15)                540       \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 16        \n",
      "=================================================================\n",
      "Total params: 3,711\n",
      "Trainable params: 3,711\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "leaky_relu_alpha =0.02\n",
    "model_mlp = Sequential()\n",
    "model_mlp.add(Dense(60, input_dim=X_train.shape[1], activation='relu'))\n",
    "# model_mlp.add(LeakyReLU(alpha=leaky_relu_alpha))\n",
    "model_mlp.add(Dense(35, kernel_initializer='normal', activation='relu'))\n",
    "# model_mlp.add(LeakyReLU(alpha=leaky_relu_alpha))\n",
    "model_mlp.add(Dense(15, kernel_initializer='normal', activation='relu'))\n",
    "# model_mlp.add(LeakyReLU(alpha=leaky_relu_alpha))\n",
    "model_mlp.add(Dense(1, activation='sigmoid'))\n",
    "model_mlp.compile(optimizer='adam', loss=f1_loss, metrics=['accuracy', f1])\n",
    "model_mlp.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback = [keras.callbacks.EarlyStopping(monitor='val_f1', min_delta=0, patience=10,\n",
    "                                        mode='auto', restore_best_weights=True),\n",
    "          keras.callbacks.ReduceLROnPlateau(monitor='val_f1', factor=0.25, patience=6,\n",
    "                                            min_lr=0.00001)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 23735 samples, validate on 10173 samples\n",
      "Epoch 1/150\n",
      "23735/23735 [==============================] - 0s 14us/step - loss: 0.4982 - accuracy: 0.8565 - f1: 0.5081 - val_loss: 0.4961 - val_accuracy: 0.8680 - val_f1: 0.5116\n",
      "Epoch 2/150\n",
      "23735/23735 [==============================] - 0s 13us/step - loss: 0.4826 - accuracy: 0.8670 - f1: 0.5213 - val_loss: 0.5124 - val_accuracy: 0.8779 - val_f1: 0.4864\n",
      "Epoch 3/150\n",
      "23735/23735 [==============================] - 0s 13us/step - loss: 0.4739 - accuracy: 0.8693 - f1: 0.5319 - val_loss: 0.4797 - val_accuracy: 0.8748 - val_f1: 0.5245\n",
      "Epoch 4/150\n",
      "23735/23735 [==============================] - 0s 13us/step - loss: 0.4688 - accuracy: 0.8710 - f1: 0.5354 - val_loss: 0.4783 - val_accuracy: 0.8572 - val_f1: 0.5253\n",
      "Epoch 5/150\n",
      "23735/23735 [==============================] - 0s 14us/step - loss: 0.4664 - accuracy: 0.8716 - f1: 0.5347 - val_loss: 0.4810 - val_accuracy: 0.8734 - val_f1: 0.5236\n",
      "Epoch 6/150\n",
      "23735/23735 [==============================] - 0s 14us/step - loss: 0.4580 - accuracy: 0.8771 - f1: 0.5422 - val_loss: 0.4674 - val_accuracy: 0.8676 - val_f1: 0.5342\n",
      "Epoch 7/150\n",
      "23735/23735 [==============================] - 0s 13us/step - loss: 0.4548 - accuracy: 0.8797 - f1: 0.5457 - val_loss: 0.4805 - val_accuracy: 0.8823 - val_f1: 0.5224\n",
      "Epoch 8/150\n",
      "23735/23735 [==============================] - 0s 14us/step - loss: 0.4518 - accuracy: 0.8783 - f1: 0.5519 - val_loss: 0.4653 - val_accuracy: 0.8670 - val_f1: 0.5394\n",
      "Epoch 9/150\n",
      "23735/23735 [==============================] - 0s 13us/step - loss: 0.4434 - accuracy: 0.8806 - f1: 0.5595 - val_loss: 0.4646 - val_accuracy: 0.8711 - val_f1: 0.5349\n",
      "Epoch 10/150\n",
      "23735/23735 [==============================] - 0s 14us/step - loss: 0.4410 - accuracy: 0.8814 - f1: 0.5621 - val_loss: 0.4648 - val_accuracy: 0.8733 - val_f1: 0.5351\n",
      "Epoch 11/150\n",
      "23735/23735 [==============================] - 0s 15us/step - loss: 0.4436 - accuracy: 0.8841 - f1: 0.5600 - val_loss: 0.4646 - val_accuracy: 0.8752 - val_f1: 0.5388\n",
      "Epoch 12/150\n",
      "23735/23735 [==============================] - 0s 16us/step - loss: 0.4394 - accuracy: 0.8836 - f1: 0.5651 - val_loss: 0.4662 - val_accuracy: 0.8772 - val_f1: 0.5330\n"
     ]
    }
   ],
   "source": [
    "mlp_history = model_mlp.fit(X_train, y_train,\n",
    "                            validation_data=(X_test, y_test),\n",
    "                            epochs=150, verbose=1, batch_size=128, callbacks=callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 33908 samples, validate on 10173 samples\n",
      "Epoch 1/150\n",
      "33908/33908 [==============================] - 0s 15us/step - loss: 0.4400 - accuracy: 0.8806 - f1: 0.5635 - val_loss: 0.4413 - val_accuracy: 0.8700 - val_f1: 0.5612\n",
      "Epoch 2/150\n",
      "33908/33908 [==============================] - 0s 13us/step - loss: 0.4329 - accuracy: 0.8822 - f1: 0.5695 - val_loss: 0.4328 - val_accuracy: 0.8787 - val_f1: 0.5696\n",
      "Epoch 3/150\n",
      "33908/33908 [==============================] - 0s 13us/step - loss: 0.4341 - accuracy: 0.8846 - f1: 0.5666 - val_loss: 0.4259 - val_accuracy: 0.8839 - val_f1: 0.5769\n",
      "Epoch 4/150\n",
      "33908/33908 [==============================] - 0s 13us/step - loss: 0.4270 - accuracy: 0.8857 - f1: 0.5746 - val_loss: 0.4401 - val_accuracy: 0.8750 - val_f1: 0.5612\n",
      "Epoch 5/150\n",
      "33908/33908 [==============================] - 0s 13us/step - loss: 0.4285 - accuracy: 0.8854 - f1: 0.5731 - val_loss: 0.4355 - val_accuracy: 0.8734 - val_f1: 0.5667\n",
      "Epoch 6/150\n",
      "33908/33908 [==============================] - 0s 12us/step - loss: 0.4266 - accuracy: 0.8873 - f1: 0.5753 - val_loss: 0.4383 - val_accuracy: 0.8722 - val_f1: 0.5646\n",
      "Epoch 7/150\n",
      "33908/33908 [==============================] - 0s 13us/step - loss: 0.4247 - accuracy: 0.8861 - f1: 0.5784 - val_loss: 0.4304 - val_accuracy: 0.8773 - val_f1: 0.5714\n",
      "Epoch 8/150\n",
      "33908/33908 [==============================] - 0s 13us/step - loss: 0.4342 - accuracy: 0.8823 - f1: 0.5662 - val_loss: 0.4303 - val_accuracy: 0.8785 - val_f1: 0.5712\n",
      "Epoch 9/150\n",
      "33908/33908 [==============================] - 0s 12us/step - loss: 0.4279 - accuracy: 0.8866 - f1: 0.5737 - val_loss: 0.4215 - val_accuracy: 0.8855 - val_f1: 0.5803\n",
      "Epoch 10/150\n",
      "33908/33908 [==============================] - 0s 13us/step - loss: 0.4268 - accuracy: 0.8880 - f1: 0.5747 - val_loss: 0.4229 - val_accuracy: 0.8930 - val_f1: 0.5805\n",
      "Epoch 11/150\n",
      "33908/33908 [==============================] - 1s 15us/step - loss: 0.4263 - accuracy: 0.8871 - f1: 0.5746 - val_loss: 0.4154 - val_accuracy: 0.8911 - val_f1: 0.5860\n",
      "Epoch 12/150\n",
      "33908/33908 [==============================] - 1s 15us/step - loss: 0.4242 - accuracy: 0.8882 - f1: 0.5779 - val_loss: 0.4288 - val_accuracy: 0.8932 - val_f1: 0.5706\n",
      "Epoch 13/150\n",
      "33908/33908 [==============================] - 0s 11us/step - loss: 0.4244 - accuracy: 0.8868 - f1: 0.5761 - val_loss: 0.4225 - val_accuracy: 0.8856 - val_f1: 0.5790\n",
      "Epoch 14/150\n",
      "33908/33908 [==============================] - 0s 12us/step - loss: 0.4230 - accuracy: 0.8850 - f1: 0.5778 - val_loss: 0.4189 - val_accuracy: 0.8907 - val_f1: 0.5849\n",
      "Epoch 15/150\n",
      "33908/33908 [==============================] - 0s 13us/step - loss: 0.4235 - accuracy: 0.8875 - f1: 0.5769 - val_loss: 0.4156 - val_accuracy: 0.8846 - val_f1: 0.5888\n",
      "Epoch 16/150\n",
      "33908/33908 [==============================] - 0s 11us/step - loss: 0.4266 - accuracy: 0.8880 - f1: 0.5747 - val_loss: 0.4131 - val_accuracy: 0.8940 - val_f1: 0.5912\n",
      "Epoch 17/150\n",
      "33908/33908 [==============================] - 0s 12us/step - loss: 0.4193 - accuracy: 0.8880 - f1: 0.5835 - val_loss: 0.4139 - val_accuracy: 0.8952 - val_f1: 0.5895\n",
      "Epoch 18/150\n",
      "33908/33908 [==============================] - 0s 11us/step - loss: 0.4193 - accuracy: 0.8871 - f1: 0.5818 - val_loss: 0.4117 - val_accuracy: 0.8884 - val_f1: 0.5900\n",
      "Epoch 19/150\n",
      "33908/33908 [==============================] - 0s 11us/step - loss: 0.4198 - accuracy: 0.8860 - f1: 0.5815 - val_loss: 0.4323 - val_accuracy: 0.8965 - val_f1: 0.5699\n",
      "Epoch 20/150\n",
      "33908/33908 [==============================] - 0s 11us/step - loss: 0.4169 - accuracy: 0.8901 - f1: 0.5854 - val_loss: 0.4073 - val_accuracy: 0.8931 - val_f1: 0.5937\n",
      "Epoch 21/150\n",
      "33908/33908 [==============================] - 0s 11us/step - loss: 0.4174 - accuracy: 0.8885 - f1: 0.5856 - val_loss: 0.4128 - val_accuracy: 0.8853 - val_f1: 0.5881\n",
      "Epoch 22/150\n",
      "33908/33908 [==============================] - 0s 11us/step - loss: 0.4190 - accuracy: 0.8872 - f1: 0.5834 - val_loss: 0.4068 - val_accuracy: 0.8931 - val_f1: 0.5957\n",
      "Epoch 23/150\n",
      "33908/33908 [==============================] - 0s 11us/step - loss: 0.4155 - accuracy: 0.8892 - f1: 0.5855 - val_loss: 0.4109 - val_accuracy: 0.8880 - val_f1: 0.5918\n",
      "Epoch 24/150\n",
      "33908/33908 [==============================] - 0s 11us/step - loss: 0.4145 - accuracy: 0.8894 - f1: 0.5869 - val_loss: 0.4117 - val_accuracy: 0.8932 - val_f1: 0.5909\n",
      "Epoch 25/150\n",
      "33908/33908 [==============================] - 0s 13us/step - loss: 0.4140 - accuracy: 0.8891 - f1: 0.5868 - val_loss: 0.4056 - val_accuracy: 0.8899 - val_f1: 0.5980\n",
      "Epoch 26/150\n",
      "33908/33908 [==============================] - 0s 12us/step - loss: 0.4156 - accuracy: 0.8898 - f1: 0.5858 - val_loss: 0.4080 - val_accuracy: 0.8927 - val_f1: 0.5941\n",
      "Epoch 27/150\n",
      "33908/33908 [==============================] - 0s 12us/step - loss: 0.4156 - accuracy: 0.8884 - f1: 0.5864 - val_loss: 0.4198 - val_accuracy: 0.8796 - val_f1: 0.5815\n",
      "Epoch 28/150\n",
      "33908/33908 [==============================] - 0s 12us/step - loss: 0.4105 - accuracy: 0.8902 - f1: 0.5912 - val_loss: 0.4086 - val_accuracy: 0.8950 - val_f1: 0.5927\n",
      "Epoch 29/150\n",
      "33908/33908 [==============================] - 0s 12us/step - loss: 0.4107 - accuracy: 0.8917 - f1: 0.5911 - val_loss: 0.4043 - val_accuracy: 0.8928 - val_f1: 0.5991\n",
      "Epoch 30/150\n",
      "33908/33908 [==============================] - 0s 12us/step - loss: 0.4081 - accuracy: 0.8913 - f1: 0.5942 - val_loss: 0.4070 - val_accuracy: 0.8921 - val_f1: 0.5926\n",
      "Epoch 31/150\n",
      "33908/33908 [==============================] - 0s 12us/step - loss: 0.4122 - accuracy: 0.8890 - f1: 0.5884 - val_loss: 0.4036 - val_accuracy: 0.8950 - val_f1: 0.5991\n",
      "Epoch 32/150\n",
      "33908/33908 [==============================] - 0s 12us/step - loss: 0.4132 - accuracy: 0.8914 - f1: 0.5878 - val_loss: 0.4190 - val_accuracy: 0.8769 - val_f1: 0.5811\n",
      "Epoch 33/150\n",
      "33908/33908 [==============================] - 0s 12us/step - loss: 0.4152 - accuracy: 0.8912 - f1: 0.5870 - val_loss: 0.4091 - val_accuracy: 0.8879 - val_f1: 0.5898\n",
      "Epoch 34/150\n",
      "33908/33908 [==============================] - 0s 12us/step - loss: 0.4140 - accuracy: 0.8893 - f1: 0.5865 - val_loss: 0.4083 - val_accuracy: 0.8960 - val_f1: 0.5923\n",
      "Epoch 35/150\n",
      "33908/33908 [==============================] - 0s 13us/step - loss: 0.4160 - accuracy: 0.8861 - f1: 0.5851 - val_loss: 0.4120 - val_accuracy: 0.8850 - val_f1: 0.5917\n",
      "Epoch 36/150\n",
      "33908/33908 [==============================] - 1s 19us/step - loss: 0.4152 - accuracy: 0.8916 - f1: 0.5852 - val_loss: 0.4176 - val_accuracy: 0.8965 - val_f1: 0.5836\n",
      "Epoch 37/150\n",
      "33908/33908 [==============================] - 0s 15us/step - loss: 0.4135 - accuracy: 0.8911 - f1: 0.5881 - val_loss: 0.4093 - val_accuracy: 0.8952 - val_f1: 0.5919\n",
      "Epoch 38/150\n",
      "33908/33908 [==============================] - 1s 15us/step - loss: 0.4141 - accuracy: 0.8909 - f1: 0.5857 - val_loss: 0.4032 - val_accuracy: 0.8925 - val_f1: 0.5983\n",
      "Epoch 39/150\n",
      "33908/33908 [==============================] - 0s 13us/step - loss: 0.4105 - accuracy: 0.8912 - f1: 0.5896 - val_loss: 0.4058 - val_accuracy: 0.8894 - val_f1: 0.5948\n",
      "Epoch 40/150\n",
      "33908/33908 [==============================] - 1s 15us/step - loss: 0.4104 - accuracy: 0.8920 - f1: 0.5922 - val_loss: 0.4086 - val_accuracy: 0.8878 - val_f1: 0.5928\n",
      "Epoch 41/150\n",
      "33908/33908 [==============================] - 0s 12us/step - loss: 0.4083 - accuracy: 0.8906 - f1: 0.5933 - val_loss: 0.4040 - val_accuracy: 0.8945 - val_f1: 0.5971\n",
      "Epoch 42/150\n",
      "33908/33908 [==============================] - 0s 12us/step - loss: 0.4067 - accuracy: 0.8915 - f1: 0.5952 - val_loss: 0.4035 - val_accuracy: 0.8893 - val_f1: 0.5975\n",
      "Epoch 43/150\n",
      "33908/33908 [==============================] - 0s 12us/step - loss: 0.4076 - accuracy: 0.8897 - f1: 0.5943 - val_loss: 0.4048 - val_accuracy: 0.8887 - val_f1: 0.5953\n",
      "Epoch 44/150\n",
      "33908/33908 [==============================] - 0s 12us/step - loss: 0.4127 - accuracy: 0.8879 - f1: 0.5885 - val_loss: 0.4176 - val_accuracy: 0.8988 - val_f1: 0.5855\n",
      "Epoch 45/150\n",
      "33908/33908 [==============================] - 0s 12us/step - loss: 0.4090 - accuracy: 0.8932 - f1: 0.5920 - val_loss: 0.4066 - val_accuracy: 0.8883 - val_f1: 0.5931\n",
      "Epoch 46/150\n",
      "33908/33908 [==============================] - 0s 14us/step - loss: 0.4085 - accuracy: 0.8908 - f1: 0.5931 - val_loss: 0.4056 - val_accuracy: 0.8917 - val_f1: 0.5971\n",
      "Epoch 47/150\n",
      "33908/33908 [==============================] - 0s 12us/step - loss: 0.4083 - accuracy: 0.8924 - f1: 0.5921 - val_loss: 0.4066 - val_accuracy: 0.8917 - val_f1: 0.5933\n",
      "Epoch 48/150\n",
      "33908/33908 [==============================] - 0s 12us/step - loss: 0.4126 - accuracy: 0.8858 - f1: 0.5875 - val_loss: 0.4171 - val_accuracy: 0.8979 - val_f1: 0.5831\n",
      "Epoch 49/150\n",
      "33908/33908 [==============================] - 0s 12us/step - loss: 0.4067 - accuracy: 0.8912 - f1: 0.5945 - val_loss: 0.3986 - val_accuracy: 0.8950 - val_f1: 0.6003\n",
      "Epoch 50/150\n",
      "33908/33908 [==============================] - 0s 11us/step - loss: 0.4053 - accuracy: 0.8917 - f1: 0.5950 - val_loss: 0.4055 - val_accuracy: 0.8974 - val_f1: 0.5944\n",
      "Epoch 51/150\n",
      "33908/33908 [==============================] - 0s 11us/step - loss: 0.4047 - accuracy: 0.8926 - f1: 0.5970 - val_loss: 0.4013 - val_accuracy: 0.8899 - val_f1: 0.5993\n",
      "Epoch 52/150\n",
      "33908/33908 [==============================] - 0s 11us/step - loss: 0.4052 - accuracy: 0.8914 - f1: 0.5962 - val_loss: 0.4053 - val_accuracy: 0.8951 - val_f1: 0.5936\n",
      "Epoch 53/150\n",
      "33908/33908 [==============================] - 0s 11us/step - loss: 0.4115 - accuracy: 0.8923 - f1: 0.5892 - val_loss: 0.4208 - val_accuracy: 0.8753 - val_f1: 0.5793\n",
      "Epoch 54/150\n",
      "33908/33908 [==============================] - 0s 13us/step - loss: 0.4062 - accuracy: 0.8884 - f1: 0.5949 - val_loss: 0.4084 - val_accuracy: 0.8857 - val_f1: 0.5943\n",
      "Epoch 55/150\n",
      "33908/33908 [==============================] - 0s 12us/step - loss: 0.4082 - accuracy: 0.8922 - f1: 0.5937 - val_loss: 0.4005 - val_accuracy: 0.8914 - val_f1: 0.5986\n",
      "Epoch 56/150\n",
      "33908/33908 [==============================] - 0s 11us/step - loss: 0.4096 - accuracy: 0.8916 - f1: 0.5907 - val_loss: 0.4087 - val_accuracy: 0.8854 - val_f1: 0.5906\n",
      "Epoch 57/150\n",
      "33908/33908 [==============================] - 0s 12us/step - loss: 0.4105 - accuracy: 0.8918 - f1: 0.5903 - val_loss: 0.4052 - val_accuracy: 0.8964 - val_f1: 0.5967\n",
      "Epoch 58/150\n",
      "33908/33908 [==============================] - 0s 11us/step - loss: 0.4094 - accuracy: 0.8922 - f1: 0.5913 - val_loss: 0.3976 - val_accuracy: 0.8950 - val_f1: 0.6018\n",
      "Epoch 59/150\n",
      "33908/33908 [==============================] - 1s 16us/step - loss: 0.4079 - accuracy: 0.8921 - f1: 0.5922 - val_loss: 0.3984 - val_accuracy: 0.8961 - val_f1: 0.6029\n",
      "Epoch 60/150\n",
      "33908/33908 [==============================] - 0s 13us/step - loss: 0.4077 - accuracy: 0.8915 - f1: 0.5920 - val_loss: 0.3976 - val_accuracy: 0.8934 - val_f1: 0.6045\n",
      "Epoch 61/150\n",
      "33908/33908 [==============================] - 0s 14us/step - loss: 0.4019 - accuracy: 0.8937 - f1: 0.5988 - val_loss: 0.3995 - val_accuracy: 0.8966 - val_f1: 0.6012\n",
      "Epoch 62/150\n",
      "33908/33908 [==============================] - 0s 13us/step - loss: 0.4061 - accuracy: 0.8927 - f1: 0.5935 - val_loss: 0.4065 - val_accuracy: 0.8989 - val_f1: 0.5949\n",
      "Epoch 63/150\n",
      "33908/33908 [==============================] - 0s 12us/step - loss: 0.4077 - accuracy: 0.8948 - f1: 0.5940 - val_loss: 0.3995 - val_accuracy: 0.8915 - val_f1: 0.6006\n",
      "Epoch 64/150\n",
      "33908/33908 [==============================] - 0s 11us/step - loss: 0.4015 - accuracy: 0.8932 - f1: 0.6001 - val_loss: 0.3952 - val_accuracy: 0.8936 - val_f1: 0.6053\n",
      "Epoch 65/150\n",
      "33908/33908 [==============================] - 0s 12us/step - loss: 0.4069 - accuracy: 0.8944 - f1: 0.5933 - val_loss: 0.3980 - val_accuracy: 0.8962 - val_f1: 0.6054\n",
      "Epoch 66/150\n",
      "33908/33908 [==============================] - 0s 12us/step - loss: 0.4098 - accuracy: 0.8902 - f1: 0.5899 - val_loss: 0.4027 - val_accuracy: 0.8938 - val_f1: 0.5954\n",
      "Epoch 67/150\n",
      "33908/33908 [==============================] - 1s 22us/step - loss: 0.4035 - accuracy: 0.8947 - f1: 0.5972 - val_loss: 0.3967 - val_accuracy: 0.8956 - val_f1: 0.6022\n",
      "Epoch 68/150\n",
      "33908/33908 [==============================] - 1s 18us/step - loss: 0.4040 - accuracy: 0.8941 - f1: 0.5969 - val_loss: 0.3979 - val_accuracy: 0.8996 - val_f1: 0.6059\n",
      "Epoch 69/150\n",
      "33908/33908 [==============================] - 0s 11us/step - loss: 0.4091 - accuracy: 0.8943 - f1: 0.5917 - val_loss: 0.3976 - val_accuracy: 0.8934 - val_f1: 0.6032\n",
      "Epoch 70/150\n",
      "33908/33908 [==============================] - 0s 11us/step - loss: 0.3972 - accuracy: 0.8955 - f1: 0.6039 - val_loss: 0.3908 - val_accuracy: 0.8983 - val_f1: 0.6093\n",
      "Epoch 71/150\n",
      "33908/33908 [==============================] - 0s 11us/step - loss: 0.3982 - accuracy: 0.8958 - f1: 0.6025 - val_loss: 0.3925 - val_accuracy: 0.9002 - val_f1: 0.6067\n",
      "Epoch 72/150\n",
      "33908/33908 [==============================] - 0s 11us/step - loss: 0.4037 - accuracy: 0.8947 - f1: 0.5983 - val_loss: 0.3903 - val_accuracy: 0.9005 - val_f1: 0.6119\n",
      "Epoch 73/150\n",
      "33908/33908 [==============================] - 0s 11us/step - loss: 0.4004 - accuracy: 0.8951 - f1: 0.6003 - val_loss: 0.3993 - val_accuracy: 0.8935 - val_f1: 0.6036\n",
      "Epoch 74/150\n",
      "33908/33908 [==============================] - 0s 11us/step - loss: 0.3977 - accuracy: 0.8956 - f1: 0.6018 - val_loss: 0.3880 - val_accuracy: 0.8994 - val_f1: 0.6125\n",
      "Epoch 75/150\n",
      "33908/33908 [==============================] - 0s 11us/step - loss: 0.4033 - accuracy: 0.8930 - f1: 0.5982 - val_loss: 0.4038 - val_accuracy: 0.8876 - val_f1: 0.5975\n",
      "Epoch 76/150\n",
      "33908/33908 [==============================] - 0s 11us/step - loss: 0.3970 - accuracy: 0.8969 - f1: 0.6045 - val_loss: 0.3976 - val_accuracy: 0.8965 - val_f1: 0.6002\n",
      "Epoch 77/150\n",
      "33908/33908 [==============================] - 0s 11us/step - loss: 0.4021 - accuracy: 0.8927 - f1: 0.5982 - val_loss: 0.3873 - val_accuracy: 0.9005 - val_f1: 0.6153\n",
      "Epoch 78/150\n",
      "33908/33908 [==============================] - 0s 11us/step - loss: 0.4007 - accuracy: 0.8971 - f1: 0.6000 - val_loss: 0.4052 - val_accuracy: 0.8889 - val_f1: 0.5949\n",
      "Epoch 79/150\n",
      "33908/33908 [==============================] - 0s 11us/step - loss: 0.3966 - accuracy: 0.8970 - f1: 0.6044 - val_loss: 0.3926 - val_accuracy: 0.8969 - val_f1: 0.6080\n",
      "Epoch 80/150\n",
      "33908/33908 [==============================] - 0s 11us/step - loss: 0.4002 - accuracy: 0.8948 - f1: 0.6001 - val_loss: 0.3898 - val_accuracy: 0.8979 - val_f1: 0.6106\n",
      "Epoch 81/150\n",
      "33908/33908 [==============================] - 0s 11us/step - loss: 0.3952 - accuracy: 0.8962 - f1: 0.6055 - val_loss: 0.3915 - val_accuracy: 0.8952 - val_f1: 0.6080\n",
      "Epoch 82/150\n",
      "33908/33908 [==============================] - 0s 11us/step - loss: 0.4013 - accuracy: 0.8971 - f1: 0.6000 - val_loss: 0.3910 - val_accuracy: 0.8960 - val_f1: 0.6110\n",
      "Epoch 83/150\n",
      "33908/33908 [==============================] - 0s 11us/step - loss: 0.4043 - accuracy: 0.8937 - f1: 0.5960 - val_loss: 0.3904 - val_accuracy: 0.8988 - val_f1: 0.6095\n",
      "Epoch 84/150\n",
      "33908/33908 [==============================] - 0s 12us/step - loss: 0.4006 - accuracy: 0.8947 - f1: 0.5998 - val_loss: 0.3911 - val_accuracy: 0.9009 - val_f1: 0.6078\n",
      "Epoch 85/150\n",
      "33908/33908 [==============================] - 0s 11us/step - loss: 0.3988 - accuracy: 0.8964 - f1: 0.6021 - val_loss: 0.3946 - val_accuracy: 0.8994 - val_f1: 0.6038\n",
      "Epoch 86/150\n",
      "33908/33908 [==============================] - 0s 11us/step - loss: 0.3978 - accuracy: 0.8969 - f1: 0.6033 - val_loss: 0.3951 - val_accuracy: 0.9006 - val_f1: 0.6044\n",
      "Epoch 87/150\n",
      "33908/33908 [==============================] - 0s 11us/step - loss: 0.3971 - accuracy: 0.8968 - f1: 0.6035 - val_loss: 0.3958 - val_accuracy: 0.8972 - val_f1: 0.6036\n",
      "Epoch 88/150\n",
      "33908/33908 [==============================] - 0s 13us/step - loss: 0.3966 - accuracy: 0.8955 - f1: 0.6036 - val_loss: 0.3835 - val_accuracy: 0.9006 - val_f1: 0.6184\n",
      "Epoch 89/150\n",
      "33908/33908 [==============================] - 0s 12us/step - loss: 0.3953 - accuracy: 0.8963 - f1: 0.6054 - val_loss: 0.3906 - val_accuracy: 0.8977 - val_f1: 0.6101\n",
      "Epoch 90/150\n",
      "33908/33908 [==============================] - 0s 13us/step - loss: 0.3948 - accuracy: 0.8951 - f1: 0.6052 - val_loss: 0.3945 - val_accuracy: 0.9004 - val_f1: 0.6052\n",
      "Epoch 91/150\n",
      "33908/33908 [==============================] - 0s 11us/step - loss: 0.3944 - accuracy: 0.8971 - f1: 0.6064 - val_loss: 0.4022 - val_accuracy: 0.9009 - val_f1: 0.5980\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 92/150\n",
      "33908/33908 [==============================] - 0s 11us/step - loss: 0.3957 - accuracy: 0.8967 - f1: 0.6044 - val_loss: 0.3877 - val_accuracy: 0.8977 - val_f1: 0.6125\n",
      "Epoch 93/150\n",
      "33908/33908 [==============================] - 0s 11us/step - loss: 0.3908 - accuracy: 0.8980 - f1: 0.6094 - val_loss: 0.3875 - val_accuracy: 0.9019 - val_f1: 0.6164\n",
      "Epoch 94/150\n",
      "33908/33908 [==============================] - 0s 12us/step - loss: 0.3930 - accuracy: 0.8984 - f1: 0.6086 - val_loss: 0.3897 - val_accuracy: 0.8988 - val_f1: 0.6092\n",
      "Epoch 95/150\n",
      "33908/33908 [==============================] - 0s 11us/step - loss: 0.3916 - accuracy: 0.8981 - f1: 0.6090 - val_loss: 0.4024 - val_accuracy: 0.8882 - val_f1: 0.5978\n",
      "Epoch 96/150\n",
      "33908/33908 [==============================] - 0s 11us/step - loss: 0.3882 - accuracy: 0.8980 - f1: 0.6120 - val_loss: 0.3887 - val_accuracy: 0.9013 - val_f1: 0.6111\n",
      "Epoch 97/150\n",
      "33908/33908 [==============================] - 0s 11us/step - loss: 0.3967 - accuracy: 0.8965 - f1: 0.6037 - val_loss: 0.3880 - val_accuracy: 0.8965 - val_f1: 0.6140\n",
      "Epoch 98/150\n",
      "33908/33908 [==============================] - 0s 11us/step - loss: 0.3943 - accuracy: 0.8967 - f1: 0.6063 - val_loss: 0.3903 - val_accuracy: 0.8973 - val_f1: 0.6104\n",
      "Epoch 99/150\n",
      "33908/33908 [==============================] - 0s 11us/step - loss: 0.3966 - accuracy: 0.8974 - f1: 0.6040 - val_loss: 0.3991 - val_accuracy: 0.8946 - val_f1: 0.6042\n",
      "Epoch 100/150\n",
      "33908/33908 [==============================] - 0s 11us/step - loss: 0.3984 - accuracy: 0.8969 - f1: 0.6016 - val_loss: 0.3899 - val_accuracy: 0.8960 - val_f1: 0.6115\n",
      "Epoch 101/150\n",
      "33908/33908 [==============================] - 0s 11us/step - loss: 0.3906 - accuracy: 0.8998 - f1: 0.6099 - val_loss: 0.3884 - val_accuracy: 0.8967 - val_f1: 0.6116\n",
      "Epoch 102/150\n",
      "33908/33908 [==============================] - 0s 11us/step - loss: 0.4019 - accuracy: 0.8991 - f1: 0.5986 - val_loss: 0.3874 - val_accuracy: 0.8978 - val_f1: 0.6149\n",
      "Epoch 103/150\n",
      "33908/33908 [==============================] - 0s 11us/step - loss: 0.3899 - accuracy: 0.8986 - f1: 0.6104 - val_loss: 0.3905 - val_accuracy: 0.9019 - val_f1: 0.6108\n",
      "Epoch 104/150\n",
      "33908/33908 [==============================] - 0s 11us/step - loss: 0.3879 - accuracy: 0.8999 - f1: 0.6139 - val_loss: 0.3843 - val_accuracy: 0.9019 - val_f1: 0.6155\n",
      "Epoch 105/150\n",
      "33908/33908 [==============================] - 0s 11us/step - loss: 0.3930 - accuracy: 0.8997 - f1: 0.6075 - val_loss: 0.3826 - val_accuracy: 0.9027 - val_f1: 0.6188\n",
      "Epoch 106/150\n",
      "33908/33908 [==============================] - 0s 12us/step - loss: 0.3947 - accuracy: 0.8970 - f1: 0.6054 - val_loss: 0.4007 - val_accuracy: 0.8892 - val_f1: 0.5989\n",
      "Epoch 107/150\n",
      "33908/33908 [==============================] - 0s 11us/step - loss: 0.3894 - accuracy: 0.8988 - f1: 0.6120 - val_loss: 0.3858 - val_accuracy: 0.9010 - val_f1: 0.6149\n",
      "Epoch 108/150\n",
      "33908/33908 [==============================] - 0s 11us/step - loss: 0.3920 - accuracy: 0.8975 - f1: 0.6083 - val_loss: 0.3863 - val_accuracy: 0.9013 - val_f1: 0.6155\n",
      "Epoch 109/150\n",
      "33908/33908 [==============================] - 0s 11us/step - loss: 0.3880 - accuracy: 0.8998 - f1: 0.6124 - val_loss: 0.3865 - val_accuracy: 0.8982 - val_f1: 0.6128\n",
      "Epoch 110/150\n",
      "33908/33908 [==============================] - 0s 11us/step - loss: 0.3966 - accuracy: 0.8963 - f1: 0.6030 - val_loss: 0.3901 - val_accuracy: 0.9012 - val_f1: 0.6094\n",
      "Epoch 111/150\n",
      "33908/33908 [==============================] - 0s 11us/step - loss: 0.3918 - accuracy: 0.8999 - f1: 0.6099 - val_loss: 0.3920 - val_accuracy: 0.8947 - val_f1: 0.6091\n",
      "Epoch 112/150\n",
      "33908/33908 [==============================] - 0s 11us/step - loss: 0.3910 - accuracy: 0.8983 - f1: 0.6095 - val_loss: 0.3858 - val_accuracy: 0.8970 - val_f1: 0.6137\n",
      "Epoch 113/150\n",
      "33908/33908 [==============================] - 0s 11us/step - loss: 0.3856 - accuracy: 0.8993 - f1: 0.6158 - val_loss: 0.3799 - val_accuracy: 0.9004 - val_f1: 0.6209\n",
      "Epoch 114/150\n",
      "33908/33908 [==============================] - 0s 11us/step - loss: 0.3880 - accuracy: 0.9008 - f1: 0.6126 - val_loss: 0.3801 - val_accuracy: 0.9002 - val_f1: 0.6225\n",
      "Epoch 115/150\n",
      "33908/33908 [==============================] - 0s 11us/step - loss: 0.3959 - accuracy: 0.8929 - f1: 0.6053 - val_loss: 0.4075 - val_accuracy: 0.8837 - val_f1: 0.5920\n",
      "Epoch 116/150\n",
      "33908/33908 [==============================] - 0s 11us/step - loss: 0.3907 - accuracy: 0.8998 - f1: 0.6102 - val_loss: 0.3875 - val_accuracy: 0.9009 - val_f1: 0.6138\n",
      "Epoch 117/150\n",
      "33908/33908 [==============================] - 0s 11us/step - loss: 0.3865 - accuracy: 0.9001 - f1: 0.6148 - val_loss: 0.3809 - val_accuracy: 0.9021 - val_f1: 0.6202\n",
      "Epoch 118/150\n",
      "33908/33908 [==============================] - 0s 11us/step - loss: 0.3857 - accuracy: 0.9016 - f1: 0.6148 - val_loss: 0.3818 - val_accuracy: 0.9038 - val_f1: 0.6206\n",
      "Epoch 119/150\n",
      "33908/33908 [==============================] - 0s 11us/step - loss: 0.3901 - accuracy: 0.9004 - f1: 0.6108 - val_loss: 0.3820 - val_accuracy: 0.9027 - val_f1: 0.6191\n",
      "Epoch 120/150\n",
      "33908/33908 [==============================] - 0s 11us/step - loss: 0.3860 - accuracy: 0.9006 - f1: 0.6150 - val_loss: 0.3968 - val_accuracy: 0.8988 - val_f1: 0.6009\n",
      "Epoch 121/150\n",
      "33908/33908 [==============================] - 0s 11us/step - loss: 0.3932 - accuracy: 0.8987 - f1: 0.6079 - val_loss: 0.3807 - val_accuracy: 0.9008 - val_f1: 0.6192\n",
      "Epoch 122/150\n",
      "33908/33908 [==============================] - 0s 11us/step - loss: 0.3904 - accuracy: 0.8997 - f1: 0.6100 - val_loss: 0.3864 - val_accuracy: 0.8988 - val_f1: 0.6145\n",
      "Epoch 123/150\n",
      "33908/33908 [==============================] - 0s 11us/step - loss: 0.3874 - accuracy: 0.8996 - f1: 0.6148 - val_loss: 0.4033 - val_accuracy: 0.8871 - val_f1: 0.5973\n",
      "Epoch 124/150\n",
      "33908/33908 [==============================] - 0s 11us/step - loss: 0.3856 - accuracy: 0.8999 - f1: 0.6158 - val_loss: 0.3787 - val_accuracy: 0.9016 - val_f1: 0.6237\n",
      "Epoch 125/150\n",
      "33908/33908 [==============================] - 0s 11us/step - loss: 0.3904 - accuracy: 0.8978 - f1: 0.6099 - val_loss: 0.3995 - val_accuracy: 0.8884 - val_f1: 0.6018\n",
      "Epoch 126/150\n",
      "33908/33908 [==============================] - 0s 11us/step - loss: 0.3901 - accuracy: 0.9001 - f1: 0.6101 - val_loss: 0.3814 - val_accuracy: 0.9009 - val_f1: 0.6179\n",
      "Epoch 127/150\n",
      "33908/33908 [==============================] - 0s 11us/step - loss: 0.3883 - accuracy: 0.8988 - f1: 0.6122 - val_loss: 0.3834 - val_accuracy: 0.8991 - val_f1: 0.6177\n",
      "Epoch 128/150\n",
      "33908/33908 [==============================] - 0s 11us/step - loss: 0.3831 - accuracy: 0.9018 - f1: 0.6180 - val_loss: 0.3819 - val_accuracy: 0.8999 - val_f1: 0.6197\n",
      "Epoch 129/150\n",
      "33908/33908 [==============================] - 0s 11us/step - loss: 0.3849 - accuracy: 0.9002 - f1: 0.6165 - val_loss: 0.3763 - val_accuracy: 0.9033 - val_f1: 0.6249\n",
      "Epoch 130/150\n",
      "33908/33908 [==============================] - 0s 11us/step - loss: 0.3860 - accuracy: 0.8994 - f1: 0.6152 - val_loss: 0.3768 - val_accuracy: 0.9046 - val_f1: 0.6228\n",
      "Epoch 131/150\n",
      "33908/33908 [==============================] - 0s 11us/step - loss: 0.3827 - accuracy: 0.9019 - f1: 0.6178 - val_loss: 0.3823 - val_accuracy: 0.9042 - val_f1: 0.6183\n",
      "Epoch 132/150\n",
      "33908/33908 [==============================] - 0s 11us/step - loss: 0.3874 - accuracy: 0.8990 - f1: 0.6133 - val_loss: 0.3820 - val_accuracy: 0.8981 - val_f1: 0.6183\n",
      "Epoch 133/150\n",
      "33908/33908 [==============================] - 0s 11us/step - loss: 0.3884 - accuracy: 0.8987 - f1: 0.6129 - val_loss: 0.3816 - val_accuracy: 0.9024 - val_f1: 0.6199\n",
      "Epoch 134/150\n",
      "33908/33908 [==============================] - 0s 11us/step - loss: 0.3883 - accuracy: 0.9001 - f1: 0.6129 - val_loss: 0.3863 - val_accuracy: 0.8981 - val_f1: 0.6139\n",
      "Epoch 135/150\n",
      "33908/33908 [==============================] - 0s 11us/step - loss: 0.3816 - accuracy: 0.9018 - f1: 0.6189 - val_loss: 0.3820 - val_accuracy: 0.8993 - val_f1: 0.6184\n",
      "Epoch 136/150\n",
      "33908/33908 [==============================] - 0s 11us/step - loss: 0.3811 - accuracy: 0.9016 - f1: 0.6201 - val_loss: 0.3901 - val_accuracy: 0.8943 - val_f1: 0.6094\n",
      "Epoch 137/150\n",
      "33908/33908 [==============================] - 0s 11us/step - loss: 0.3850 - accuracy: 0.9008 - f1: 0.6161 - val_loss: 0.3801 - val_accuracy: 0.9009 - val_f1: 0.6203\n",
      "Epoch 138/150\n",
      "33908/33908 [==============================] - 0s 11us/step - loss: 0.3788 - accuracy: 0.9023 - f1: 0.6219 - val_loss: 0.3864 - val_accuracy: 0.9051 - val_f1: 0.6166\n",
      "Epoch 139/150\n",
      "33908/33908 [==============================] - 0s 11us/step - loss: 0.3855 - accuracy: 0.9001 - f1: 0.6147 - val_loss: 0.3770 - val_accuracy: 0.9018 - val_f1: 0.6253\n",
      "Epoch 140/150\n",
      "33908/33908 [==============================] - 0s 11us/step - loss: 0.3821 - accuracy: 0.9011 - f1: 0.6189 - val_loss: 0.3829 - val_accuracy: 0.8962 - val_f1: 0.6176\n",
      "Epoch 141/150\n",
      "33908/33908 [==============================] - 0s 11us/step - loss: 0.3839 - accuracy: 0.9005 - f1: 0.6172 - val_loss: 0.3832 - val_accuracy: 0.9038 - val_f1: 0.6157\n",
      "Epoch 142/150\n",
      "33908/33908 [==============================] - 0s 11us/step - loss: 0.3835 - accuracy: 0.9017 - f1: 0.6175 - val_loss: 0.3789 - val_accuracy: 0.9032 - val_f1: 0.6234\n",
      "Epoch 143/150\n",
      "33908/33908 [==============================] - 0s 11us/step - loss: 0.3796 - accuracy: 0.9031 - f1: 0.6214 - val_loss: 0.3957 - val_accuracy: 0.9051 - val_f1: 0.6059\n",
      "Epoch 144/150\n",
      "33908/33908 [==============================] - 0s 11us/step - loss: 0.3869 - accuracy: 0.8992 - f1: 0.6138 - val_loss: 0.3783 - val_accuracy: 0.8993 - val_f1: 0.6204\n",
      "Epoch 145/150\n",
      "33908/33908 [==============================] - 0s 11us/step - loss: 0.3816 - accuracy: 0.9029 - f1: 0.6192 - val_loss: 0.3846 - val_accuracy: 0.9043 - val_f1: 0.6155\n",
      "Epoch 146/150\n",
      "33908/33908 [==============================] - 0s 11us/step - loss: 0.3857 - accuracy: 0.9003 - f1: 0.6144 - val_loss: 0.3921 - val_accuracy: 0.9050 - val_f1: 0.6086\n",
      "Epoch 147/150\n",
      "33908/33908 [==============================] - 0s 11us/step - loss: 0.3815 - accuracy: 0.9020 - f1: 0.6192 - val_loss: 0.3791 - val_accuracy: 0.9054 - val_f1: 0.6215\n",
      "Epoch 148/150\n",
      "33908/33908 [==============================] - 0s 11us/step - loss: 0.3863 - accuracy: 0.9009 - f1: 0.6142 - val_loss: 0.3888 - val_accuracy: 0.8948 - val_f1: 0.6129\n",
      "Epoch 149/150\n",
      "33908/33908 [==============================] - 0s 11us/step - loss: 0.3888 - accuracy: 0.9008 - f1: 0.6126 - val_loss: 0.3768 - val_accuracy: 0.9018 - val_f1: 0.6230\n",
      "Epoch 150/150\n",
      "33908/33908 [==============================] - 0s 11us/step - loss: 0.3823 - accuracy: 0.9014 - f1: 0.6179 - val_loss: 0.3711 - val_accuracy: 0.9069 - val_f1: 0.6326\n"
     ]
    }
   ],
   "source": [
    "mlp_history = model_mlp.fit(train.drop('labels', axis=1), train['labels'],\n",
    "                            validation_data=(X_test, y_test),\n",
    "                            epochs=150, verbose=1, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0], dtype=int32)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_mlp.predict_classes(test).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    9766\n",
       "1    1537\n",
       "Name: labels, dtype: int64"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame({'labels':model_mlp.predict_classes(test).flatten()\n",
    "             })['labels'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({\n",
    "    'labels':model_mlp.predict_classes(test).flatten()\n",
    "             }).to_excel('Submission_3.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dedupe_kernel",
   "language": "python",
   "name": "dedupe_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
